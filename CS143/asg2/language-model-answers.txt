Zac Plante
ASG2

1. What are the 15 most frequent words and bigrams for each category, and what are their frequencies?
	The 15 most common words in the positive data are food(124), 's(88), great(83), place(61), restaurant(55), service(54), good(53), best(43), excellent(38), one(37), n't(37), menu(35), time(34), wonderful(33), and atmosphere(29).
	The 15 most common words in the negative data are food(149), restaurant(96), us(95), service(89), n't(88), 's(82), one(72), place(68), table(60), would(57), good(53), back(51), go(49), never(45), and experience(43).
	The 15 most common bigrams in the positive data are great food(13), food excellent(12), great place(9), food service(8), highly recommend(8), food great(8), food wonderful(6), wine list(6), recommend place(6), food good(6), wonderful service(5), service friendly(5), worth trip(5), go back(5), and every time(5).
	The 15 most common bigrams in the negative data are dining experience(12), go back(10), prime rib(8), number one(8), coral grill(8), food cold(7), looked like(6), n't know(6), come back(6), wait staff(6), told us(6), n't even(6), service food(6), n't get(6), and 20 minutes(6).

2. What are the collocations that were found for each category?
	The collocations in the positive data are chez capo; highly recommend; pancake house; san francisco; mashed potatoes; wine list; millbrae pancake; rosa negra; several times; worth trip; big city; food excellent; sure try; head chef; something
everyone; ala carte; eastern market; outdoor patio; ravioli browned; great food
	The collocations in the negative data are prime rib; coral grill; dining experience; fried rice; number one; crab legs; taco bell; tourist trap; local boys; needless say; looked like; 227 bistro; speak manager; health department; sunset restaurant; wait staff; medium rare; pattio area; food cold; come back

3. Consider the normalized version of the first sentence of the training data. Given the frequency distributions you created during steps 2 and 3, calculate by hand the probability of that sentence, using a bigram model. Show your work.
	The probability "excellent restaurant" is equal to the P(excellent)XP(restaurant|excellent). There are 6558 words in the positive data, while excellent appears 38 times, so P(excellent)=38/6558. Restaurant appears after excellent twice, and since we know excellent appears 38 times P(restaurant|excellent)=1/19. So the probability of this sentence is 38/(6558*19).

4. Consider again the first sentence of the training data, but without stopwords removed. What is the probability of this sentence using a trigram model. You do not need to calculate the number. Just write out the equation with the probabilities you would need to calculate it. What order of Markov Assumption is this model using? What would be order of the Markov assumption for a 4-gram model?
	For a trigram model of the first sentence, the probability would be P(resturant|excellent,an), or P(An)XP(excellent|an)X(restaurant|excellent). We already know the last one, but we would need to know how many times an appears, as well as how many times excellent appears after an to calculate it fully. This is 2-order Markov Chain, while a 4-gram would be a 3-order one.


5. Calculate by hand P (mashed âˆª potatoes) within the positive domain. Show your work.
	Mashed appears 7 times, while potatoes appears 10. So P(mashed +P(potatoes)= 17/6558. However, mashed potatoes appears once, so P(mashed potatoes)=1/6558, so 17-1/6558=16/6558.

6. What happens if you encounter a word that is not in your frequency tables when calculating the probability of an unseen sentence (a sentence that is not in your training data)?
	We have two options when this occurs. The first is to ignore the word and hope that the law of large numbers help smooth it over. Alternatively, we can use a special unknown character <UNK> in place of it.

7. A higher order n-gram (4-gram, 5-gram and so on) model is a better language model than a bi-gram or tri-gram model. Would you say this statement is correct? Please state your reasons.
	I would say this statement is incorrect. While higher order n-grams can store more complex data and contexts, it leads to more space data. By using smaller n-grams, we cans till have information on the context, while having more varied data we can extrapolate from.